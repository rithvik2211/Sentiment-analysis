{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "71395ece",
   "metadata": {},
   "source": [
    "## Sentiment Analysis of Blogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2890f693",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c368139d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funtion converts the given string list into lower case\n",
    "def all_lower(my_list):\n",
    "    return [x.lower() for x in my_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "672ed50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funtion gives the count of syllables in a word\n",
    "def syllables(word):\n",
    "    count = 0\n",
    "    vowels = 'aeiouy'\n",
    "    word = word.lower()\n",
    "    if word[0] in vowels:\n",
    "        count +=1\n",
    "    for index in range(1,len(word)):\n",
    "        if word[index] in vowels and word[index-1] not in vowels:\n",
    "            count +=1\n",
    "    if word.endswith('es'):\n",
    "        count -= 1\n",
    "    if word.endswith('ed'):\n",
    "        count -= 1\n",
    "    if count == 0:\n",
    "        count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b8fba1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#counts personal pronouns in the given text\n",
    "def count_personal_pronouns(text):\n",
    "  pronoun_count = re.compile(r'\\b(I|we|my|ours|(?-i:us))\\b', re.I)\n",
    "  pronouns = pronoun_count.findall(text)\n",
    "  return len(pronouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db19ce2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reads all the files in Master Dictonary folder\n",
    "def words(filename):\n",
    "    di = 'MasterDictionary\\\\'\n",
    "    filename = di+ filename\n",
    "    temp = open(filename, \"r\")\n",
    "    words = temp.read().splitlines()\n",
    "    temp.close()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8bcc00d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['StopWords_Auditor.txt', 'StopWords_Currencies.txt', 'StopWords_DatesandNumbers.txt', 'StopWords_Generic.txt', 'StopWords_GenericLong.txt', 'StopWords_Geographic.txt', 'StopWords_Names.txt']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# folder path\n",
    "dir_path = r'C:\\Users\\sanda\\Downloads\\Sentiment Analysis\\StopWords'\n",
    "\n",
    "# list to store files\n",
    "file_list = []\n",
    "\n",
    "# Iterate directory\n",
    "for path in os.listdir(dir_path):\n",
    "    # check if current path is a file\n",
    "    if os.path.isfile(os.path.join(dir_path, path)):\n",
    "        file_list.append(path)\n",
    "print(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ce3b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"Input.xlsx\")\n",
    "#df.head()\n",
    "#for privacy reasons I haven't shown the blogs i have took "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "477f2fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n",
      "114\n"
     ]
    }
   ],
   "source": [
    "print(df.iat[0,0])\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48feef52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37.txt\n",
      "38.txt\n",
      "39.txt\n",
      "40.txt\n",
      "41.txt\n",
      "42.txt\n",
      "43.txt\n",
      "page not found\n",
      "44.txt\n",
      "45.txt\n",
      "46.txt\n",
      "47.txt\n",
      "48.txt\n",
      "49.txt\n",
      "50.txt\n",
      "51.txt\n",
      "52.txt\n",
      "53.txt\n",
      "54.txt\n",
      "55.txt\n",
      "56.txt\n",
      "page not found\n",
      "57.txt\n",
      "58.txt\n",
      "59.txt\n",
      "60.txt\n",
      "61.txt\n",
      "62.txt\n",
      "63.txt\n",
      "64.txt\n",
      "65.txt\n",
      "66.txt\n",
      "67.txt\n",
      "68.txt\n",
      "69.txt\n",
      "70.txt\n",
      "71.txt\n",
      "72.txt\n",
      "73.txt\n",
      "74.txt\n",
      "75.txt\n",
      "76.txt\n",
      "77.txt\n",
      "78.txt\n",
      "79.txt\n",
      "80.txt\n",
      "81.txt\n",
      "82.txt\n",
      "83.txt\n",
      "84.txt\n",
      "85.txt\n",
      "86.txt\n",
      "87.txt\n",
      "88.txt\n",
      "89.txt\n",
      "90.txt\n",
      "91.txt\n",
      "92.txt\n",
      "93.txt\n",
      "94.txt\n",
      "95.txt\n",
      "96.txt\n",
      "97.txt\n",
      "98.txt\n",
      "99.txt\n",
      "100.txt\n",
      "101.txt\n",
      "102.txt\n",
      "103.txt\n",
      "104.txt\n",
      "105.txt\n",
      "106.txt\n",
      "107.txt\n",
      "108.txt\n",
      "109.txt\n",
      "110.txt\n",
      "111.txt\n",
      "112.txt\n",
      "113.txt\n",
      "114.txt\n",
      "115.txt\n",
      "116.txt\n",
      "117.txt\n",
      "118.txt\n",
      "119.txt\n",
      "120.txt\n",
      "121.txt\n",
      "122.txt\n",
      "123.txt\n",
      "124.txt\n",
      "125.txt\n",
      "126.txt\n",
      "127.txt\n",
      "128.txt\n",
      "129.txt\n",
      "130.txt\n",
      "131.txt\n",
      "132.txt\n",
      "133.txt\n",
      "134.txt\n",
      "135.txt\n",
      "136.txt\n",
      "137.txt\n",
      "138.txt\n",
      "139.txt\n",
      "140.txt\n",
      "141.txt\n",
      "142.txt\n",
      "143.txt\n",
      "page not found\n",
      "144.txt\n",
      "145.txt\n",
      "146.txt\n",
      "147.txt\n",
      "148.txt\n",
      "149.txt\n",
      "150.txt\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(df)):\n",
    "    try:\n",
    "        page = urlopen(df.iat[i,1])\n",
    "    except:\n",
    "        print(\"page not found\")\n",
    "        \n",
    "    html = page.read().decode(\"utf-8\")\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    \n",
    "    url_id = df.iat[i,0]\n",
    "    doc_name = str(url_id) + (\".txt\")\n",
    "    doc = 'Text files\\\\' + doc_name\n",
    "    print(doc_name)\n",
    "\n",
    "    f = open(doc, \"w\",encoding=\"utf-8\")\n",
    "    for data in soup.find_all(\"p\"):\n",
    "        sum = data.get_text()\n",
    "        f.writelines(sum)\n",
    " \n",
    "    f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3823c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stpwords = []\n",
    "\n",
    "for name in file_list:\n",
    "    io = 'StopWords\\\\'\n",
    "    name  = io + name\n",
    "    stpword = open(name , \"r\")\n",
    "    temp = stpword.read().splitlines()\n",
    "    myList = [i.split('|')[0] for i in temp]\n",
    "    stpword.close()\n",
    "    stpwords = stpwords + myList\n",
    "    \n",
    "\n",
    "stpwords = all_lower(stpwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7dd5226b",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_wrds = words(\"positive-words.txt\")\n",
    "negative_wrds = words(\"negative-words.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea009b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame(columns = [\"URL_ID\",\"URL\",\"POSITIVE SCORE\", \"NEGATIVE SCORE\", \"POLARITY SCORE\", \"SUBJECTIVITY SCORE\", \"AVG SENTENCE LENGTH\", \"PERCENTAGE OF COMPLEX WORDS\", \"FOG INDEX\", \"AVG NUMBER OF WORDS PER SENTENCE\", \"COMPLEX WORD COUNT\", \"WORD COUNT\", \"SYLLABLE PER WORD\", \"PERSONAL PRONOUNS\", \"AVG WORD LENGTH\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8002a241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [URL_ID, URL, POSITIVE SCORE, NEGATIVE SCORE, POLARITY SCORE, SUBJECTIVITY SCORE, AVG SENTENCE LENGTH, PERCENTAGE OF COMPLEX WORDS, FOG INDEX, AVG NUMBER OF WORDS PER SENTENCE, COMPLEX WORD COUNT, WORD COUNT, SYLLABLE PER WORD, PERSONAL PRONOUNS, AVG WORD LENGTH]\n",
       "Index: []"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c42884ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(df)):\n",
    "    url_id = df.iat[i,0]\n",
    "    doc_name = str(url_id) + (\".txt\")\n",
    "    filename = 'Text files\\\\' + doc_name\n",
    "    temp = open(filename, \"r\",encoding=\"utf-8\")\n",
    "    text = temp.read()\n",
    "    temp.close()\n",
    "    \n",
    "    word_tokens = word_tokenize(text)\n",
    "    sentence = all_lower(word_tokens)\n",
    "    puntuations = ['?', '!', '.', ',','\"','-']\n",
    "        \n",
    "    no_puntuation_sentence = []\n",
    "    for w in sentence:\n",
    "        if w not in puntuations:\n",
    "            no_puntuation_sentence.append(w)\n",
    "            \n",
    "\n",
    "\n",
    "    filtered_sentence = []\n",
    "    for w in no_puntuation_sentence:\n",
    "        if w not in stpwords:\n",
    "            filtered_sentence.append(w)\n",
    "    \n",
    "    sent_tokens = sent_tokenize(text)\n",
    "   \n",
    "    total_sylb_count =0\n",
    "    for a in filtered_sentence:\n",
    "        temp = syllables(a)\n",
    "        total_sylb_count += temp\n",
    "        \n",
    "    total_char_count =0\n",
    "\n",
    "    for a in filtered_sentence:\n",
    "        temp = len(a)\n",
    "        total_char_count += temp\n",
    "        \n",
    "    pos = len([e for e in filtered_sentence if e in positive_wrds])\n",
    "    neg = len([e for e in filtered_sentence if e in negative_wrds])\n",
    "    polarity = (pos - neg) /((pos+neg) +0.000001)\n",
    "    subjectivity_score = (pos+neg)/(len(filtered_sentence)+0.000001)\n",
    "    complex_wrd_count = len([e for e in filtered_sentence if syllables(e) >2])\n",
    "    cleaned_wrd_count = len(filtered_sentence)\n",
    "    \n",
    "    try:\n",
    "        avg_sent_len = cleaned_wrd_count/ len(sent_tokens)\n",
    "        avg_wrds_sent_len = len(no_puntuation_sentence)/len(sent_tokens)\n",
    "        percentage_cmplx_wrds = complex_wrd_count/ cleaned_wrd_count\n",
    "        sylb_count_perwrd = total_sylb_count/ cleaned_wrd_count\n",
    "        avg_wrd_len = total_char_count/len(no_puntuation_sentence)\n",
    "    except:\n",
    "        avg_sent_len = 0\n",
    "        avg_wrds_len = 0\n",
    "        avg_wrds_sent_len =0\n",
    "        percentage_cmplx_wrds = 0\n",
    "        sylb_count_perwrd = 0\n",
    "        avg_wrd_len = 0\n",
    "        \n",
    "    \n",
    "    fog_index =0.4*(avg_sent_len + percentage_cmplx_wrds)\n",
    "    total_pronoun_count = count_personal_pronouns(text)\n",
    "    \n",
    "    \n",
    "    output.loc[i,\"URL_ID\"] = df.iat[i,0]\n",
    "    output.loc[i,\"URL\"] = df.iat[i,1]\n",
    "    output.loc[i,\"POSITIVE SCORE\"] = pos\n",
    "    output.loc[i,\"NEGATIVE SCORE\"] = neg\n",
    "    output.loc[i,\"POLARITY SCORE\"] = polarity\n",
    "    output.loc[i,\"SUBJECTIVITY SCORE\"] = subjectivity_score\n",
    "    output.loc[i,\"AVG SENTENCE LENGTH\"] = avg_sent_len\n",
    "    output.loc[i,\"PERCENTAGE OF COMPLEX WORDS\"] = percentage_cmplx_wrds\n",
    "    output.loc[i,\"FOG INDEX\"] = fog_index\n",
    "    output.loc[i,\"AVG NUMBER OF WORDS PER SENTENCE\"] = avg_wrds_sent_len\n",
    "    output.loc[i,\"COMPLEX WORD COUNT\"] = complex_wrd_count\n",
    "    output.loc[i,\"WORD COUNT\"] = cleaned_wrd_count\n",
    "    output.loc[i,\"SYLLABLE PER WORD\"] = sylb_count_perwrd\n",
    "    output.loc[i,\"PERSONAL PRONOUNS\"] = total_pronoun_count\n",
    "    output.loc[i,\"AVG WORD LENGTH\"] = avg_wrd_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58e7280d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_excel(\"output.xlsx\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb47f61d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
